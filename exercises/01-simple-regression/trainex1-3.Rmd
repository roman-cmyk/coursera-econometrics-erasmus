---
title: 'Week 1: Simple Regression, Training Exercise 1.3'
author: "Anthony Nguyen"
subtitle: <h3>Coursera/Erasumus U., Econometric Methods and Applications</h3>
output:
  pdf_document: default
    
    
---
```{r load packages, include=FALSE}
library(tidyverse)
```

```{r load data for exercise, include=FALSE}
TrainExer13 <- read_tsv("../../data/TrainExer13.txt")
```

#Questions  
Dataset `TrainExer13` contains the winning times (W) of the Olympic 100-meter finals (for men) from 1948 to 2004.
The calendar years 1948-2004 are transformed to games (G) 1-15 to simplify computations. A simple regression
model for the trend in winning times is $W_i = \alpha + \beta G_i + \varepsilon_i$.  
  
  (a) Compute a and b, and determine the values of $R^2$ and $s$.  
    
    
  (b) Are you confident on the predictive ability of this model? Motivate your answer.  
  
    
  (c) What prediction do you get for 2008, 2012, and 2016? Compare your predictions with the actual winning times.
  
***  
    
#Answers  
  
*(a) Compute $a$ (intercept) and $b$ (slope), and determine the values of $R^2$ and $s$.*
  
$$
b = \frac{\sum_{i=1}^n (x_i-\bar{x})(y_i-\bar{y})}{\sum_{i=1}^n (x_i-\bar{x})^2}
$$  
```{r calculate xbar, echo=TRUE}
#we can use the "Year" column for `x`.
xbar <- mean(TrainExer13$Game)
xbar
```
```{r calculate ybar, echo=TRUE}
ybar <- mean(TrainExer13$`Winning time men`)
ybar
```
```{r sub in values and compute b, echo = TRUE}
b <- sum((TrainExer13$Game - xbar)*(TrainExer13$`Winning time men` - ybar)) / sum((TrainExer13$Game - xbar)^2)

b
```
$$
a = \bar{y} - b\bar{x}
$$ 
```{r sub in values and compuate a, echo = TRUE}
a <- ybar - b*xbar

a
```
Next, to calculate $R^2$ and $s$, we can use the following formulas:  
  
$$
e_i=y_i-a-bx_i
$$
```{r calculate vector e, echo = FALSE}
e <- TrainExer13$`Winning time men`-a-(b*TrainExer13$Game)
```

$R^2$ = (sum of squares explained)/(sum of squares total), or:  
$$
R^2 = 1- \frac{\sum_{i=1}^n e^2}{\sum_{i=1}^n (y_i-\bar{y})^2}
$$
```{r sub in values, compute R^2, echo = TRUE}
R2 <- 1 - (sum(e^2) / sum((TrainExer13$`Winning time men`- ybar)^2))
R2
```
And finally for $s$, the *residual standard error*, we can use:
$$
s^2 = \frac{1}{n-2} \sum_{i=1}^n e_i^2
$$
```{r sub in values, compute s^2, echo = TRUE}
s2 <- (1/(length(e)-2))*(sum(e^2))
s2
s <- sqrt(s2)
s
```
  
 *(b) Are you confident on the predictive ability of this model? Motivate your answer.*  
   
I would not be very confident about this model's ability to predict times, as I don't think that the year of the race is a good explanatory variable for faster winning times.  Undoubtedly there are more factors that go into this.  

 *(c) What prediction do you get for 2008, 2012, and 2016? Compare your predictions with the actual winning times.*
  
Based our model, we can predict the winning times in subsequent olympic years using the forumula:
$$
y = a + bx
$$
```{r sub in values for x to make predictions, echo = TRUE}
y2008 <- a + b*(16) 
y2012 <- a + b*(17) 
y2016 <- a + b*(18)

y2008
y2012
y2016
```
A quick search on the internet, and we find that the actual results for the event in those years was:  
  
2008: 9.69  
2012: 9.63  
2016: 9.81  

***

Just to check, let's run the regression using the baseR `lm` function.  
```{r run regression using baseR, echo = FALSE}
lm_winning <- lm(`Winning time men`~Game, data = TrainExer13)
summary(lm_winning)
```

```{r plot lm_winning, echo = FALSE}
TrainExer13 %>% ggplot(aes(x = Game, y = `Winning time men`)) +
  geom_point() +
  geom_smooth(method="lm", se = FALSE)
```

