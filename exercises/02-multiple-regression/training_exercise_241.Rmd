---
title: 'Week 2: Multiple Regression, Training Exercises'
subtitle: <h3>Coursera/Erasumus U., Econometric Methods and Applications</h3>
author: "Anthony Nguyen"
output:
  pdf_document: 
    keep_tex: no
    includes:
      in_header: preamble_2.tex
  html_document:
    theme: cosmo
---
\makeatletter
\g@addto@macro\bfseries{\boldmath}
\makeatother

#Training Exercise 2.4.1  
  
##Questions  
  
By solving the questions of this exercise, you provide a proof of the Gauss-Markov theorem. We use the following notation:  
  
+ The OLS estimator is $b = A_0y$, where $A_0 = (X'X)^{-1}X'$.  
+ Let $\hat{\beta} = Ay$ be linear, unbiased, with $A (k\; \text{x}\; n)$ matrix.  
+ Define the difference matrix $D = A - A_0$.  
  
###(a) Prove the following three results:   
  
(i) $var(\hat{\beta}) = \sigma^2 AA'$.  
(ii) $\hat{\beta}$ unbiased implies $AX = I$ and $DX = 0$.  
(iii) Part (ii) implies $AA' = DD' + (X'X)^{-1}$.
        
###(b) Prove that part (a-iii) implies $var(\hat{\beta}) = \sigma^2 DD'$.  
  
###(c) Prove that part (b) implies $var(\hat{\beta}) - var(b)$ is positive semidefinite (Gauss-Markov).  
  
###(d) Prove that $var(\hat{\beta}) \geq var(b_j)$ for every $j = 1, \dots, k$.  
  
***
\pagebreak  
  
##Answers    
    
###(a) Prove the following three results:  
  
(i) $var(\hat{\beta}) = \sigma^2 AA'$.  
  
Given that $y = X\beta + \varepsilon$, then we find that:  
\begin{align*}  
\hat{\beta} &= Ay \\  
  &= A(X\beta + \varepsilon) \\ 
  &= AX\beta + A\varepsilon  
\end{align*}  

Now given that $A$, $X$, and $\beta$ are fixed and $E(\varepsilon) = 0$, we find that:  
\begin{align*}  
E(\hat{\beta}) &= AX\beta + AE(\varepsilon) \\  
  &= AX\beta \\  
var(\hat{\beta}) &= E((\hat{\beta} - E(\hat{\beta})) (\hat{\beta} - E(\hat{\beta}))') \\  
  &= E(A\varepsilon (A\varepsilon)') \\  
  &= E(A\varepsilon \varepsilon'A') \\  
  &= AE(\varepsilon \varepsilon')A' \\  
  &= A\sigma^2 IA' \\  
  &= \sigma^2 AA'  
\end{align*}

(ii) $\hat{\beta}$ unbiased implies $AX = I$ and $DX = 0$.    
  
$E(\hat{\beta})$ is unbiased if $AX\beta = \beta$ for all $\beta$, or:  
\begin{align*}  
DX &= (A - A_0)X \\  
  &= AX - A_0 X \\  
  &= I - (x'x)^{-1} X'X \\  
  &= I - I \\  
  &= 0 
\end{align*}
    
(iii) Part (ii) implies $AA' = DD' + (X'X)^{-1}$.  
\begin{align*}  
AA' &= (D + A_0)(D + A_0)' \\  
  &= DD' + A_0 D' + DA_0' + A_0 A_0' \\  
\\  
DA_0' &= DX(X'X)^{-1} = 0\cdot(X'X)^{-1} = 0 \\
\\  
A_0 D' &= (DA_0')^{-1} = 0 \\  
\\  
AA' &= DD' + A_0 A_0' \\  
  &= DD' + (X'X)^{-1} X'X (X'X)^{-1} \\  
  &= DD' + (X'X)^{-1}  
\end{align*}
          
###(b) Prove that part (a-iii) implies $var(\hat{\beta}) = \sigma^2 DD'$.  
  
Given that $var(b) = \sigma^2(X'X)^{-1}$, we find that:  
\begin{align*}  
var(\hat{\beta}) &= \sigma^2 AA' \\  
  &= \sigma^2 (DD' + (X'X)^{-1}) \\  
  &= \sigma^2 DD' + \sigma^2 (X'X)^{-1} \\  
  &= \sigma^2 DD' + var(b)
\end{align*}
  
###(c) Prove that part (b) implies $var(\hat{\beta}) - var(b)$ is positive semidefinite (Gauss-Markov).  
  
From (b) we've seen that $var(\hat{\beta}) - var(b) = \sigma^2 DD'$.  Given that $\sigma^2 > 0$, then it remains to be shown that $DD'$ is positive semidefinite (p.s.d.):  
  
$D$ is a $(k\; \text{x}\; n)$ matrix  
$D'$ is a $(n\; \text{x}\; n)$ matrix  
$c$ is a $(n\; \text{x}\; 1)$ vector  
$d = D'c$ is a $(n\; \text{x}\; 1)$ vector with components $d_1, \dots, d_n$ 
\begin{align*}  
\therefore c'DD'c &= (D'c)' D'c \\  
  &= d'd \\  
  &= \sum_{i=1}^{n}d_i^2 \geq 0 \quad \text{proving that $DD'$ is p.s.d.}   
\end{align*}  

###(d) Prove that $var(\hat{\beta}) \geq var(b_j)$ for every $j = 1, \dots, k$.  
  
Let $c_j$ be the $(k\; \text{x}\; 1)$ $j^{th}$ unit vector, which means $c_j$ can be re-written as:  
\begin{align*}  
c_j &= \begin{bmatrix} 
  0 \\  
  \vdots \\  
  0 \\  
  1 \\  
  0 \\  
  \vdots \\  
  0  
  \end{bmatrix}  
  \quad \text{with the single element $1$ at the $j^{th}$ entry}
\end{align*}
from $c$ we obtain that:  
$$
c_j' (var(\hat{\beta}) - var(b)) c_j = \sigma^2 c_j' DD' c_j \geq 0
$$
hence: 
$$    
c_j' var(\hat{\beta})c_j \geq c_j'var(b)c_j
$$
or equivalently:
$$
var(c_j' \hat{\beta}) \geq var(c_j'b)
$$
As $c_j'\hat{\beta} = \hat{\beta_j}$ and $c_j'b = b_j$, it follows that:  
$$
var(\hat{\beta_j}) \geq var(b_j) \quad \text{for every $j = 1, \dots, n$}
$$